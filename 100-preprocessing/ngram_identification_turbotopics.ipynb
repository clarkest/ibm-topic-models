{
 "metadata": {
  "name": "",
  "signature": "sha256:f967b45fc77a34c945f8b7a5ef9031200625abd200d00882379930b823072603"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_dir = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/\"\n",
      "list_dir = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-topic-model/100-preprocessing/lists/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first, we need to aggregate the docs and put in a single file\n",
      "filnam = doc_dir + \"world-manager-bigrams.tsv\"\n",
      "world_docs = list(csv.reader(open(filnam, 'rb'), delimiter='\\t'))\n",
      "\n",
      "filnam = doc_dir + \"values-managers-bigrams.tsv\"\n",
      "val_docs = list(csv.reader(open(filnam, 'rb'), delimiter='\\t'))\n",
      "\n",
      "# first, pull all of the text fields from both jams into a single list\n",
      "docs = list()\n",
      "for doc in world_docs:\n",
      "    docs.append(doc[17])\n",
      "\n",
      "for doc in val_docs:\n",
      "    docs.append(doc[18])\n",
      "\n",
      "docs_file = doc_dir + \"combined_docs.csv\"\n",
      "with open(fil_out, 'wb') as csv_file:\n",
      "    for doc in docs[1:]:\n",
      "        csv_file.write(doc + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# point to wherever you've stashed turbotopics, and load its compute_ngrams module\n",
      "import sys\n",
      "sys.path.append('/Users/clarkbernier/sandbox/turbotopics-master/')\n",
      "import compute_ngrams\n",
      "\n",
      "pvalue = 0.0001\n",
      "use_perm = None\n",
      "out_filename = list_dir + \"tt_ngrams_0001.txt\"\n",
      "compute_ngrams.main(docs_file, pvalue, use_perm, out_filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pare the file down to just the repeated n grams\n",
      "tt_filename = list_dir + \"tt_ngrams_0001.txt\"\n",
      "tt_repeats_filename = list_dir + \"tt_repeated_ngrams_0001.txt\"\n",
      "\n",
      "tt = list(csv.reader(open(tt_filename, 'rb'), delimiter='|'))\n",
      "with open(tt_repeats_filename, 'wb') as csv_file:\n",
      "    for item in tt:\n",
      "        if int(item[1]) > 1:\n",
      "            if \" \" in item[0]:\n",
      "                csv_file.write(item[0] + \", \" + item[1] + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the straight ngram finder gives us a lot of instances with 1 or more stop words and only 1 non-stop\n",
      "# for the most part these are boring ngrams, and we'd prefer there be at least two non-stops to \n",
      "# count as an ngram.  Right now, we use the turbotopics stops with a few stops removed that seemed particularly relevant\n",
      "# for IBM ngrams (up, down, together).  We also are uninterested in ngrams ending in a stop word, as these trailing \n",
      "# stops tend to add no meaning to the preceding ngram\n",
      "stop_words = \"/Users/clarkbernier/sandbox/turbotopics-master/stop_words_up_down_together.txt\"\n",
      "# it's useful to compare the list of ngrams to those ngrams that would have been removed using a larger stop list\n",
      "compare_stop_words = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/en.txt\"\n",
      "\n",
      "out_filename = list_dir + \"ngrams_minus_stops_and_ends.txt\"\n",
      "dropped_filename = list_dir + \"tt_ngrams_dropped_by_compare.txt\"\n",
      "\n",
      "tt = list(csv.reader(open(tt_filename, 'rb'), delimiter=','))\n",
      "sw = list(csv.reader(open(stop_words, 'rb'), delimiter=','))\n",
      "compare_sw = list(csv.reader(open(compare_stop_words, 'rb'), delimiter=','))\n",
      "\n",
      "r_friendly_sw = list()\n",
      "r_friendly_compare_sw = list()\n",
      "\n",
      "kept_tt = list()\n",
      "for line in sw:\n",
      "    r_friendly_sw.append(line[0])\n",
      "for line in compare_sw:\n",
      "    r_friendly_compare_sw.append(line[0])\n",
      "    \n",
      "# only keep if it has more than 2 non-stop words AND doesn't end in a stop\n",
      "with open(out_filename, 'wb') as csv_file:\n",
      "    for item in tt:\n",
      "        #print item\n",
      "        non_stop_count = 0\n",
      "        for wd in item[0].split():\n",
      "            if wd not in r_friendly_sw:\n",
      "                non_stop_count += 1\n",
      "        if non_stop_count > 1 and item[0].split()[-1] not in r_friendly_sw:\n",
      "            csv_file.write(item[0] + \", \" + item[1] + \"\\n\")\n",
      "            kept_tt.append(item)\n",
      "\n",
      "\n",
      "with open(dropped_filename, 'wb') as csv_file:\n",
      "    for item in kept_tt:\n",
      "    #print item\n",
      "        non_stop_count = 0\n",
      "        for wd in item[0].split():\n",
      "            if wd not in r_friendly_compare_sw:\n",
      "                non_stop_count += 1\n",
      "        if item[0].split()[-1] in r_friendly_sw:\n",
      "            csv_file.write(item[0] + \", \" + item[1] + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# append to charles's phrases file (because I used the post-processed docs for building up the vocab, a few ngrams \n",
      "#    from that file are probably missed in the above process)\n",
      "with open(list_dir + phrases.txt\", 'rb') as fil:\n",
      "    phrases = fil.read().splitlines() \n",
      "with open(list_dir + ngrams_minus_stops_and_ends.txt\", 'rb') as fil:\n",
      "    ngrams = list(csv.reader(fil, delimiter=','))\n",
      "\n",
      "for ng in ngrams:\n",
      "    if int(ng[1]) >= 10:\n",
      "        phrases.append(ng[0])\n",
      "       \n",
      "# we need this sorted by the longest phrases so that they get processed before their sub-phrases\n",
      "phrases_out = sorted(phrases, key=lambda phrase: phrase.count(\" \"), reverse=True)\n",
      "with open(list_dir + \"new_phrases.txt\", 'wb') as fil:\n",
      "    fil.writelines(map(lambda x:x+\"\\n\", phrases_out))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}