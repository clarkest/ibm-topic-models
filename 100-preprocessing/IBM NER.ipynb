{
 "metadata": {
  "name": "",
  "signature": "sha256:7ece0233e4b05351d85e54216aa21afbae1ead58677201664c79fd0986359650"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I haven't fully updated this file to be compatible with the directory moves.  It should all work, but needs a little more manual reconfiguration rather than changing a few variables at the beginning.  I think it's useful to have around if we want to revisit the NER approach, but since the turbotopics ngrams seem to be more useful for now, I'm holding off updating this [CJB 2015-07-13]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag import StanfordNERTagger\n",
      "import csv\n",
      "import os\n",
      "import pickle\n",
      "start_count = 0\n",
      "os.environ['JAVAHOME'] = \".:/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/bin\"\n",
      "\n",
      "ner_model = '/Users/clarkbernier/sandbox/stanford-ner-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz'\n",
      "ner_jar = \"/Users/clarkbernier/sandbox/stanford-ner-2015-04-20/stanford-ner.jar\"\n",
      "\n",
      "st = StanfordNERTagger(ner_model,ner_jar)\n",
      "\n",
      "filnam = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/world-manager-bigrams.tsv\"\n",
      "world_docs = list(csv.reader(open(filnam, 'rb'), delimiter='\\t'))\n",
      "\n",
      "filnam = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/values-managers-bigrams.tsv\"\n",
      "val_docs = list(csv.reader(open(filnam, 'rb'), delimiter='\\t'))\n",
      "\n",
      "# first, pull all of the text fields from both jams into a single list\n",
      "docs = list()\n",
      "for doc in world_docs:\n",
      "    docs.append(doc[17])\n",
      "\n",
      "for doc in val_docs:\n",
      "    docs.append(doc[18])\n",
      "\n",
      "#print docs[1]\n",
      "\n",
      "big_doc = \"\"\n",
      "# flip through each doc and tag it\n",
      "count = start_count\n",
      "for doc in docs[start_count:]:\n",
      "    count += 1\n",
      "    big_doc += (\"\\n\" + doc)\n",
      "    # every 5000 docs, drop the pickled dictionary\n",
      "    if (count % 5000 == 0): \n",
      "        tagged_docs = st.tag(big_doc.split())\n",
      "        out_file = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/tagged_docs/{}.pkl\".format(count)\n",
      "        with open(out_file, 'wb') as output_file:\n",
      "            pickle.dump(tagged_docs, output_file)\n",
      "        big_doc = \"\"\n",
      "tagged_docs = st.tag(big_doc.split())\n",
      "out_file = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/tagged_docs/{}.pkl\".format(count)\n",
      "with open(out_file, 'wb') as output_file:\n",
      "    pickle.dump(tagged_docs, output_file)       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import os \n",
      "\n",
      "tokens = list()\n",
      "tagged_docs_path = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/tagged_docs/\"\n",
      "for f in os.listdir(tagged_docs_path):\n",
      "    if f[0:1] != \".\":\n",
      "        print f\n",
      "        tagged_docs_fil = tagged_docs_path + f \n",
      "        with open(tagged_docs_fil, 'rb') as in_file:\n",
      "            tokens_in = pickle.load(in_file)\n",
      "        tokens = tokens + tokens_in\n",
      "print len(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000.pkl\n",
        "15000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40342.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5000.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3904919"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "import operator\n",
      "import csv\n",
      "# iterate over tagged tokens, creating a list of all categorized 1-gram tokens\n",
      "tagged_tokens = list()\n",
      "tagged_n_grams = list()\n",
      "last_category=\"O\"\n",
      "cumulative_n_gram = \"\"\n",
      "truly_n_gram = False\n",
      "punctu = set(string.punctuation)\n",
      "#punctu.remove(\"'\")\n",
      "for token in tokens:\n",
      "    if token[1] == \"O\":\n",
      "        if truly_n_gram:\n",
      "            tagged_n_grams.append((cumulative_n_gram, last_category))\n",
      "            truly_n_gram = False\n",
      "        last_category = \"O\"\n",
      "    else:\n",
      "        # strip out punctuation\n",
      "        temp_token = token[0]\n",
      "        temp_token = \"\".join(x for x in temp_token if x not in punctu)\n",
      "        tagged_tokens.append((temp_token, token[1]))\n",
      "        if token[1] == last_category:\n",
      "            cumulative_n_gram += (\" \" + temp_token)\n",
      "            truly_n_gram = True\n",
      "        else: \n",
      "            cumulative_n_gram = temp_token\n",
      "            last_category = token[1]\n",
      "            truly_n_gram = False\n",
      "token_counts = dict()\n",
      "token_cats = dict()\n",
      "just_n_grams = dict()\n",
      "just_n_gram_cats = dict()\n",
      "for tt in tagged_tokens:\n",
      "    if tt[0] in token_counts:\n",
      "        token_counts[tt[0]] += 1\n",
      "    else:\n",
      "        token_counts[tt[0]] = 1\n",
      "        token_cats[tt[0]] = tt[1]\n",
      "for tt in tagged_n_grams:\n",
      "    #print tt[0]\n",
      "    if tt[0] in token_counts:\n",
      "        token_counts[tt[0]] += 1\n",
      "        just_n_grams[tt[0]] += 1\n",
      "    else:\n",
      "        token_counts[tt[0]] = 1\n",
      "        token_cats[tt[0]] = tt[1]\n",
      "        just_n_grams[tt[0]] = 1\n",
      "        just_n_gram_cats[tt[0]] = tt[1]\n",
      "        \n",
      "\n",
      "repeated_n_grams = dict()\n",
      "for n_gram in just_n_grams:\n",
      "    if just_n_grams[n_gram] > 1:\n",
      "        # print n_gram + \": \" + str(just_n_grams[n_gram])\n",
      "        repeated_n_grams[n_gram] = just_n_grams[n_gram]\n",
      "sorted_n_grams = sorted(repeated_n_grams.items(), key=operator.itemgetter(1), reverse=True)\n",
      "\n",
      "fil_out = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/ner_n_grams.csv\"\n",
      "with open(fil_out, 'wb') as csv_file:\n",
      "    writer = csv.writer(csv_file)\n",
      "    for key, value in sorted_n_grams:\n",
      "        writer.writerow([key, value])\n",
      "print len(sorted_n_grams)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1033\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now go through this list and count how many occurences there are in the text if we lower case everything\n",
      "n_gram_count = dict()\n",
      "for n_gram in sorted_n_grams:\n",
      "    indx = ' '.join(n_gram[0].lower().split())\n",
      "    n_gram_count[indx] = 0\n",
      "for doc in docs:\n",
      "    #remove puncuation\n",
      "    punctu = set(string.punctuation)\n",
      "    temp_doc = \"\".join(x for x in doc if x not in punctu)\n",
      "    temp_doc = ' '.join(temp_doc.split())\n",
      "    temp_doc = temp_doc.lower()\n",
      "    temp_doc = temp_doc.decode('utf-8')\n",
      "    for n_gram in sorted_n_grams:\n",
      "        n_gram_lookup = n_gram[0].lower()\n",
      "        n_gram_lookup = ' '.join(n_gram_lookup.split())\n",
      "        if n_gram_lookup in temp_doc:\n",
      "            n_gram_count[n_gram_lookup] += 1\n",
      "        \n",
      "sorted_n_grams = sorted(n_gram_count.items(), key=operator.itemgetter(1), reverse=True)\n",
      "fil_out = \"/Users/clarkbernier/Dropbox/IBM Local/ibm-code/preprocessing/ner_n_grams_full.csv\"\n",
      "with open(fil_out, 'wb') as csv_file:\n",
      "    writer = csv.writer(csv_file)\n",
      "    for key, value in sorted_n_grams:\n",
      "        if value > 1:\n",
      "            writer.writerow([key, value])\n",
      "print len(sorted_n_grams)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1033\n"
       ]
      }
     ],
     "prompt_number": 16
    }
   ],
   "metadata": {}
  }
 ]
}