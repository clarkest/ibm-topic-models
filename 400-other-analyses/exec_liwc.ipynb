{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31329"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "os.chdir(\"/Users/clarkbernier/Box Sync/IBM Local/ibm-topic-model/library/liwc_js\")\n",
    "%run jensen_shannon.py\n",
    "docs = []\n",
    "csv.field_size_limit(93000000)\n",
    "#with open(\"../../outputs/exec_set_docs.csv\", 'r') as csvfile: \n",
    "# with gzip.open(\"../../outputs/exec_comps_set_docs.gz\", 'r') as csvfile: \n",
    "with gzip.open(\"../../outputs/all_docs_for_liwc.gz\", 'r') as csvfile: \n",
    "    csv_file = csv.DictReader(csvfile)\n",
    "    for row in csv_file:\n",
    "        docs.append(row)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1e9f4ebc9b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjust_bigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_term_count_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjust_bigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliwc_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mliwc_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_processed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabsize' is not defined"
     ]
    }
   ],
   "source": [
    "targets = [\"quote_set\", \"responded_set\", \"early_set\", \"exec_set\", \"mgr_set\", \"non_mgr_set\" \"asia_set\", \"us_set\", \"euro_set\"]\n",
    "just_bigs = {}\n",
    "\n",
    "for idx, msg in enumerate(docs):\n",
    "    key = msg['user']\n",
    "    if key in targets:\n",
    "        just_bigs[key] = msg\n",
    "\n",
    "dists = {}\n",
    "for key in just_bigs.keys():\n",
    "    dists[key] = get_term_count_distribution([just_bigs[key]], vocabsize=vocabsize, liwc_map=liwc_map, pre_processed=pre_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pre process all docs\n",
    "vocabsize=4000\n",
    "sampling=False \n",
    "sampsize=1000\n",
    "verbose=False\n",
    "pre_processed=False\n",
    "liwc_map=True\n",
    "\n",
    "segments2docs = {}\n",
    "doc_segmentation_fnc=lambda x: [x['user']]\n",
    "documents = docs\n",
    "keys = {}\n",
    "\n",
    "for idx, msg in enumerate(documents):\n",
    "    if verbose:\n",
    "        sys.stderr.write('\\r') ; sys.stderr.write('msg %s' % idx) ; sys.stderr.flush()\n",
    "    key = doc_segmentation_fnc(msg)[0]\n",
    "    segments2docs[key] = msg\n",
    "if verbose: sys.stderr.write('\\n')\n",
    "\n",
    "# For each segment, get count distribution over terms:\n",
    "dists = {}\n",
    "for key in segments2docs.keys():\n",
    "    print key\n",
    "    dists[key] = get_term_count_distribution([segments2docs[key]], vocabsize=vocabsize, liwc_map=liwc_map, pre_processed=pre_processed)\n",
    "\n",
    "\n",
    "#return distances\n",
    "\n",
    "# Now measure pairwise distances:\n",
    "# dist_from_exec = {}\n",
    "# for key in dists.keys():\n",
    "#    dist_from_exec[key] = jensen_shannon(dists[key], dists[\"quote_set\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asia_set\n",
      "quote_set\n",
      "non_mgr_set\n",
      "comp98\n",
      "comp99\n",
      "comp94\n",
      "comp95\n",
      "comp96\n",
      "comp97\n",
      "comp90\n",
      "comp91\n",
      "comp92\n",
      "comp93\n",
      "comp14\n",
      "comp15\n",
      "comp16\n",
      "comp17\n",
      "comp10\n",
      "comp11\n",
      "comp12\n",
      "comp13\n",
      "us_set\n",
      "comp18\n",
      "comp19\n",
      "comp76\n",
      "comp77\n",
      "comp74\n",
      "comp75\n",
      "comp72\n",
      "comp73\n",
      "comp70\n",
      "comp71\n",
      "comp78\n",
      "comp79\n",
      "comp89\n",
      "comp88\n",
      "comp83\n",
      "comp82\n",
      "comp81\n",
      "comp80\n",
      "comp87\n",
      "comp86\n",
      "comp85\n",
      "comp84\n",
      "comp65\n",
      "comp64\n",
      "comp67\n",
      "comp66\n",
      "comp61\n",
      "comp60\n",
      "comp63\n",
      "comp62\n",
      "comp69\n",
      "comp68\n",
      "responded_set\n",
      "euro_set\n",
      "comp50\n",
      "comp51\n",
      "comp52\n",
      "comp53\n",
      "comp54\n",
      "comp55\n",
      "comp56\n",
      "comp57\n",
      "comp58\n",
      "comp59\n",
      "mgr_set\n",
      "comp49\n",
      "comp48\n",
      "comp47\n",
      "comp46\n",
      "comp45\n",
      "comp44\n",
      "comp43\n",
      "comp42\n",
      "comp41\n",
      "comp40\n",
      "comp38\n",
      "comp39\n",
      "comp32\n",
      "comp33\n",
      "comp30\n",
      "comp31\n",
      "comp36\n",
      "comp37\n",
      "comp34\n",
      "comp35\n",
      "comp2\n",
      "comp3\n",
      "comp1\n",
      "comp6\n",
      "comp7\n",
      "comp4\n",
      "comp5\n",
      "comp8\n",
      "comp9\n",
      "exec_set\n",
      "comp29\n",
      "comp28\n",
      "comp21\n",
      "comp20\n",
      "comp23\n",
      "comp22\n",
      "comp25\n",
      "comp24\n",
      "comp27\n",
      "comp26\n",
      "comp100\n",
      "early_set\n"
     ]
    }
   ],
   "source": [
    "vocabsize=4000\n",
    "sampling=False \n",
    "sampsize=1000\n",
    "verbose=False\n",
    "pre_processed=False\n",
    "liwc_map=True\n",
    "\n",
    "segments2docs = {}\n",
    "doc_segmentation_fnc=lambda x: [x['user']]\n",
    "documents = docs\n",
    "keys = {}\n",
    "\n",
    "for idx, msg in enumerate(documents):\n",
    "    if verbose:\n",
    "        sys.stderr.write('\\r') ; sys.stderr.write('msg %s' % idx) ; sys.stderr.flush()\n",
    "    key = doc_segmentation_fnc(msg)[0]\n",
    "    segments2docs[key] = msg\n",
    "if verbose: sys.stderr.write('\\n')\n",
    "\n",
    "# For each segment, get count distribution over terms:\n",
    "dists = {}\n",
    "for key in segments2docs.keys():\n",
    "    print key\n",
    "    dists[key] = get_term_count_distribution([segments2docs[key]], vocabsize=vocabsize, liwc_map=liwc_map, pre_processed=pre_processed)\n",
    "\n",
    "\n",
    "#return distances\n",
    "\n",
    "# Now measure pairwise distances:\n",
    "# dist_from_exec = {}\n",
    "# for key in dists.keys():\n",
    "#    dist_from_exec[key] = jensen_shannon(dists[key], dists[\"quote_set\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caluculate an Overall Center\n",
    "seg2docs_copy = segments2docs.copy()\n",
    "dud = seg2docs_copy.pop(\"exec_set\")\n",
    "mega_comment = {'text':\" \".join([d['text'] for d in seg2docs_copy.values()])}\n",
    "overall_center = get_term_count_distribution([mega_comment], vocabsize=vocabsize, liwc_map=liwc_map, pre_processed=pre_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'quote_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d8696b2365ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjensen_shannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quote_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjensen_shannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exec_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjensen_shannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mgr_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjensen_shannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quote_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exec_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjensen_shannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quote_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mgr_set\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quote_set'"
     ]
    }
   ],
   "source": [
    "print(jensen_shannon(dists[\"quote_set\"], overall_center))\n",
    "print(jensen_shannon(overall_center, dists[\"exec_set\"]))\n",
    "print(jensen_shannon(overall_center, dists[\"mgr_set\"]))\n",
    "print(jensen_shannon(dists[\"quote_set\"], dists[\"exec_set\"]))\n",
    "print(jensen_shannon(dists[\"quote_set\"], dists[\"mgr_set\"]))\n",
    "print(jensen_shannon(dists[\"exec_set\"], dists[\"mgr_set\"]))\n",
    "print(jensen_shannon(overall_center, dists[\"asia_set\"]))\n",
    "print(jensen_shannon(overall_center, dists[\"euro_set\"]))\n",
    "print(jensen_shannon(overall_center, dists[\"us_set\"]))\n",
    "print(jensen_shannon(dists[\"quote_set\"], dists[\"asia_set\"]))\n",
    "print(jensen_shannon(dists[\"quote_set\"], dists[\"euro_set\"]))\n",
    "print(jensen_shannon(dists[\"quote_set\"], dists[\"us_set\"]))\n",
    "print(jensen_shannon(dists[\"quote_set\"], dists[\"early_set\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31329"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25693:8\n",
      "17430:3\n",
      "28697:8\n",
      "20607:5\n",
      "28795:9\n",
      "28791:7\n",
      "28347:7\n",
      "23931:9\n",
      "4101:7\n",
      "3803:9\n",
      "20499:4\n",
      "29455:8\n",
      "13338:8\n",
      "28944:8\n",
      "15667:6\n",
      "13475:8\n",
      "21886:8\n",
      "30774:6\n",
      "25281:8\n",
      "19228:8\n",
      "24272:7\n",
      "24187:9\n",
      "13942:5\n",
      "29088:9\n",
      "2117:9\n",
      "14793:5\n",
      "19:9\n",
      "21860:7\n",
      "16186:9\n",
      "448:8\n",
      "13495:8\n",
      "18635:8\n",
      "26964:9\n",
      "26274:8\n",
      "503:0\n",
      "11925:9\n",
      "16382:8\n",
      "15114:9\n",
      "1323:8\n",
      "7234:9\n",
      "9565:5\n",
      "24744:8\n",
      "27214:7\n",
      "25506:9\n",
      "31044:8\n",
      "31047:9\n",
      "30718:7\n",
      "29195:9\n",
      "26704:7\n",
      "11112:7\n"
     ]
    }
   ],
   "source": [
    "# Persist all the docs\n",
    "with open('../../outputs/liwc_sets/liwc_doc_distributions.csv','wb') as f:    \n",
    "    w = csv.writer(f)    \n",
    "    w.writerow([\"liwc.cat\",\"doc.num\",\"prev\"])\n",
    "    for k in dists:\n",
    "        # if len(dists[k]) < 10: print str(k) + \":\" + str(len(dists[k]))\n",
    "        for liwc_key in dists[k]:\n",
    "            w.writerow([liwc_key, k, dists[k].get(liwc_key)])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the Overall Center and the other Target Centers\n",
    "\n",
    "targets = [\"quote_set\", \"responded_set\", \"early_set\", \"exec_set\", \"mgr_set\", \"non_mgr_set\", \"asia_set\", \"us_set\", \"euro_set\"]\n",
    "\n",
    "out_set = {}\n",
    "for key in overall_center:\n",
    "    ret_set = [overall_center[key]]\n",
    "    for target_key in targets:\n",
    "        ret_set.append(dists[target_key].get(key) or 0)\n",
    "    out_set[key] = ret_set\n",
    "with open('../../outputs/liwc_sets/liwc_distributions.csv','wb') as f:    \n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"liwc.cat\",\"overall.dist\"] + targets)\n",
    "    for k,v in out_set.items():\n",
    "       w.writerow([k] + list(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quote_set\n",
      "responded_set\n",
      "early_set\n",
      "exec_set\n",
      "mgr_set\n",
      "non_mgr_set\n",
      "asia_set\n",
      "us_set\n",
      "euro_set\n",
      "overall_center\n",
      "comp87\n",
      "comp22\n"
     ]
    }
   ],
   "source": [
    "# Now measure and output the pairwise distances\n",
    "dists[\"overall_center\"] = overall_center\n",
    "targets = [\"quote_set\", \"responded_set\", \"early_set\", \"exec_set\", \"mgr_set\",\"non_mgr_set\", \"asia_set\", \"us_set\", \"euro_set\", \"overall_center\", \"comp87\", \"comp22\"]\n",
    "\n",
    "dist_set = {}\n",
    "for target_key in targets:\n",
    "    print target_key\n",
    "    for key in dists.keys():\n",
    "        dist_set[key] = jensen_shannon(dists[key], dists[target_key])\n",
    "    fil_nam = '../../outputs/liwc_sets/dists_from_%s.csv' % (target_key) \n",
    "    with open(fil_nam,'wb') as f:    \n",
    "        w = csv.writer(f)\n",
    "        headr = \"dist.from.%s\" % (target_key)\n",
    "        w.writerow([\"user\",\"dist\"])\n",
    "        w.writerows(dist_set.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target_key = targets[2]\n",
    "key = dists.keys()[1]\n",
    "print key\n",
    "print target_key\n",
    "dist_set = {}\n",
    "dist_set[key] = jensen_shannon(dists[key], dists[target_key])\n",
    "\n",
    "print dist_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do the execs look like?\n",
    "print(dists[\"exec_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(just_bigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_bigs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overall_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_from_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
